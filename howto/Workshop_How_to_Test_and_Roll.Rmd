---
title: "How to Test & Roll"
subtitle: "Profit-Maximizing A/B Tests"
author: "Elea McDonnell Feit & Ron Berman"
date: "7/16/2020"
output: 
  ioslides_presentation: default
  beamer_presentation: default
  powerpoint_presentation: default
widescreen: yes
---

```{r setup, include=FALSE}
library(rstan)
library(dplyr)
```

## A/B Testing  
![](images/ab_testing.png){width=75%}  

## Holdout Testing  
![](images/holdout_testing.png){width=75%}  

## Workshop materials  
The R Markdown file that created these slides is at XXX.  

Advanced R users are welcome to open the file in R Studio and run the code as we go along.   
- Requires `rstan` and `dplyr` packages  
- Requires a file with some additional functions I wrote  

Those unfamiliar with R markdown may prefer to simply follow the slides.  

## Workshop Goals
You may know how to plan and analyze an A/B test using hypothesis testing. (If you don't, check out the R-Ladies Philly recording of my previous workshop on A/B testing.)

In this session, I'd like to introduce you to an alternative way of planning and analyzing an A/B test, which is based on *Bayesian decision theory*. I think this approach has a lot of advantages, so I gave it a name: **Test & Roll**. 

So, let's start by learning how to analyze an A/B test using *Bayesian inference.* 

# Bayesian Analysis for A/B Tests

## Bayesian Analysis for A/B Tests
The data from an A/B test comparing the time users spend on your website for two versions of the homepage is in the data frame `test_data`. A summary of the data looks like this: 
```{r, echo=FALSE}
# Simulate data
set.seed(19104)
group <- c(rep("A", 500), rep("B", 500)) 
time_on_site <- c(rnorm(500, mean=5.2, sd=2), rnorm(500, mean=5.4, sd=2.2))
test_data <- data.frame(group, time_on_site)
rm(group, time_on_site)
```
```{r, echo=TRUE, message=FALSE}
test_data %>% 
  group_by(group) %>% summarize(mean=mean(time_on_site), sd=sd(time_on_site), n=n())
```
It looks like the B version keeps users on the site a bit longer, but how sure are we that B produces longer visits on average?  

## Prior beliefs
I would like to know what the mean time-on-site is for the A group and the B group. 

Before I saw this data, I knew nothing about how long people might spend on this website. They might stay for 5 seconds or they might stay for 5 hours.  

Formally, I can describe my prior beliefs with a *prior distribution*: 
$$\textrm{mean time-on-site for group} \sim N(0, 100^2)$$  

## Prior beliefs  
We can also draw a picture of that prior.
```{r, echo=FALSE}
plot(x=-300:300, y=dnorm(-300:300, mean=0, sd=100), 
     type="l", col="gray", xlab="mean time-on-site (m)", ylab="prior density")
```


## Posterior beliefs
Bayes rule tells you how you should update your beliefs after you see some data. This is easiest to see with a picture.
```{r, echo=FALSE}
n_A <- sum(test_data$group=="A")
n_B <- sum(test_data$group=="B")
s <- sd(test_data$time_on_site)
post_mean_A <- mean(test_data[test_data$group=="A", "time_on_site"])
post_mean_B <- mean(test_data[test_data$group=="B", "time_on_site"])
post_sd_A <- (1/100^2 + n_A/s^2)^-(1/2)
post_sd_B <- (1/100^2 + n_B/s^2)^-(1/2)
plot(x=(450:600)/100, y=dnorm((450:600)/100, mean=post_mean_A, sd=post_sd_A), 
     type="l", col="blue", xlab="mean time-on-site (m)", ylab="posterior density")
lines(x=(450:600)/100, y=dnorm((450:600)/100, mean=post_mean_B, sd=post_sd_B), col="red")
lines(x=(450:600)/100, y=dnorm((450:600)/100, mean=0, sd=100), col="gray")
legend("topright", col=c("blue", "red", "gray"), legend=c("posterior for A", "posterior for B", "prior"), bty="n", lty=1)
```

## Model details (mathematically)
 If we assume that the time-on-site is also distributed normally: 
$$\textrm{time-on-site} \sim \mathcal{N}(\textrm{mean time-on-site for group}, s^2)$$
Then Bayes rule tells us that the posterior distribution for mean time-on-site for each group should be: 
$$\textrm{mean time-on-site for group given data} \sim \mathcal{N}\left(\overline{y}, \left(\frac{1}{100^2} + \frac{n}{s^2}\right)^{-1}\right)$$  

I'm skipping the derivation. Hope you don't mind!

## Code for posterior updating
```{r, eval=FALSE}
n_A <- sum(test_data$group=="A") # obs for A
n_B <- sum(test_data$group=="B") # obs for B
s <- sd(test_data$time_on_site) # standard deviation of data (approx)

# Posterior mean is just the mean for each group
post_mean_A <- mean(test_data[test_data$group=="A", "time_on_site"])
post_mean_B <- mean(test_data[test_data$group=="B", "time_on_site"])

# Posterior standard deviation follows this formula
post_sd_A <- (1/100^2 + n_A/s^2)^-(1/2)
post_sd_B <- (1/100^2 + n_B/s^2)^-(1/2)
```

## Credible intervals for groups
If you like intervals, we can compute a 95% credible intervals for each group, by cutting off the left and right 2.5% of the distribution.  In this case, our posterior is normal, so we use the `qnorm()` function, which computes quantiles of the normal distribution.

```{r}
qnorm(c(0.025, 0.975), mean=post_mean_A, sd=post_sd_A) # CI for A
qnorm(c(0.025, 0.975), mean=post_mean_B, sd=post_sd_B) # CI for B
```

## Posterior for the difference between A and B
We can also compute the posterior distribution for the difference between the mean for the B group and the mean for the A group. 
```{r, echo=FALSE}
post_mean_diff <- post_mean_B - post_mean_A
post_sd_diff <- sqrt(post_sd_B^2 + post_sd_A^2)
plot(x=(-50:60)/100, y=dnorm((-50:60)/100, mean=post_mean_diff, sd=post_sd_diff), 
     type="l", col="black", 
     xlab="difference in mean time-on-site (m)", ylab="posterior density")
abline(v=0)
text(-0.25, 2.9, "A has higher mean time-on-site")
text(0.35, 2.9, "B has higher mean time-on-site")
```

## Posterior for the difference between A and B
Since the posterior for A and the posterior for B are both normal, the posterior for the difference is also normal with mean and standard deviation: 
```{r, eval=FALSE}
post_mean_diff <- post_mean_B - post_mean_A
post_sd_diff <- sqrt(post_sd_B^2 + post_sd_A^2)
```

Once we have the distribution for the difference in the mean time-on-site, we can compute the probability that the mean of B is greater than the mean of A. 
```{r}
1-pnorm(0, mean=post_mean_diff, sd=post_sd_diff)
```
It is up to the decision maker to decide whether they would like to use version B knowing that there is a 63% change that B is better than A. This depends on how costly it is to deploy B. 

## More on prior beliefs
Many people get hung up onpriors. **Don't!**

As you get more data, the posterior becomes more and more influenced by the data. In most practical situations you have enough data that, the priors do not affect the analysis much.

If you have prior information, priors are a principalled way to bring it in. 

Priors are also useful when planning an A/B test (more later).

## More on prior beliefs
When we don't want our priors to influence the outcome, we use "flat" priors. Our prior puts nearly equal weight on 4.5 minutes versus 6 minutes, so it is pretty flat.   
```{r, echo=FALSE}
plot(x=(450:600)/100, y=dnorm((450:600)/100, mean=post_mean_A, sd=post_sd_A), 
     type="l", col="blue", xlab="mean time-on-site (m)", ylab="posterior density")
lines(x=(450:600)/100, y=dnorm((450:600)/100, mean=post_mean_B, sd=post_sd_B), col="red")
lines(x=(450:600)/100, y=dnorm((450:600)/100, mean=0, sd=100), col="gray")
legend("topright", col=c("blue", "red", "gray"), legend=c("A", "B", "prior"), bty="n", lty=1)
```

## More on prior beliefs
In this analysis I used the same prior for both A and B because I know nothing about A and B.   

*Important*: using the same prior for A and B is *not the same* as assuming A and B have the same mean time-on-site. 

You can always use priors that reflect your (justifiable) prior beliefs. For instance, if A is a discount and B is no discount and our outcome is how much you purchase, then I'm pretty sure A will be as good as or better than B. 


## Summary of Bayesian Analysis for A/B tests
1. Pick treatments and an outcome to measure
2. Specify you beliefs about the outcome (prior distribution)
3. Randomly assign A and B to users and record outcome
4. Update your beliefs according to Bayes rule (posterior distribution)
5. Plot your updated belief distribution; compute intervals and probabilities
6. Make a decision or **go to 3**

## Questions? 

## Planning an A/B test

## Typical A/B email test setup screen
![](images/CampaignMonitorTestSetup.png){width=75%}

## A/B test planning as a decision problem
### Test
Choose $n_1^*$ and $n_2^*$ customers to send the treatments.  
Collect data on response.  

### Roll
Choose a treatment to deploy to the remaining $N - n_1^* - n_2^*$.  

### Objective
Maximize combined profit for test stage and the roll stage.   

## Profit-maximizing sample size
If you have a test where the response is  
$y \sim \mathcal{N}(m_A, s)$ for group A and $y \sim \mathcal{N}(m_B, s)$ for group B  

and your priors are  
($m_1, m_2 \sim N(\mu, \sigma)$)  

the profit maximizing sample size is:  
$$n_1 = n_2 = \sqrt{\frac{N}{4}\left( \frac{s}{\sigma} \right)^2 + \left( \frac{3}{4} \left( \frac{s}{\sigma} \right)^2  \right)^2 } -  \frac{3}{4} \left(\frac{s}{\sigma} \right)^2$$
This new sample size formula is derived in [Feit and Berman (2019) *Marketing Science*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3274875).

## I made an R function to compute the sample size 

```{r}
source("nn_functions.R")
mu = 0.68  # average conversion rate
s = mu*(1-mu) # binomial approximation
sigma = 0.03
N=100000
(n <- test_size_nn(N=N, s=s, mu=mu, sigma=0.03))
```

## How does this work?
Bigger population ($N$) $\rightarrow$ bigger test  

## How does this work?
More noise in the repsonse ($s$) $\rightarrow$ bigger test  

## How does this work?
More prior difference between treatments ($\sigma$) $\rightarrow$ smaller test 



```{r, echo=FALSE}
generate_syn_expt <- function(nexpt, nobs, mu, sigma, omega) {
  nobs <- matrix(nobs, nrow=nexpt, ncol=2) # equal test sizes
  y <- matrix(NA, nrow=nexpt, ncol=2)
  for (e in 1:nexpt) {
    t <- rnorm(1, mean=mu, sd=omega)
    m <- rnorm(2, mean=t, sd=sigma)
    while (min(t,m) < 0 | max(t,m) > 1) { # reject values outside [0,1] 
      t <- rnorm(1, mean=mu, sd=omega)
      m <- rnorm(2, mean=t, sd=sigma)
    }
    y[e, 1] <- mean(rnorm(nobs[e,1], mean=m[1], sd=sqrt(m[1]*(1-m[1]))))
    y[e, 2] <- mean(rnorm(nobs[e,2], mean=m[2], sd=sqrt(m[2]*(1-m[2]))))
  }
  list(nexpt=nexpt, y=y, nobs=nobs)
}
set.seed(19980103)
expts <- generate_syn_expt(nexpt=100, nobs=100000, 
                           mu=0.676, sigma=0.030, omega=0.199) # pop estimates from paper
```

## Come up with priors {.smaller}
**Hierarchical Stan model for past experiments**
```{stan, output.var="lr"}
// Stan code for Lewis and Rao 2015 data
// L&R only report the mean and standard deviation for the control group for each experiment
data {
  int<lower=1> nexpt; // number of experiments
  real<lower=2> nobs[nexpt]; // sample size for control group
  real ybar[nexpt]; // observed mean for control group
  real<lower=0> s[nexpt]; // observed standard deviation for experiment (pooled)
}
parameters {
  real m[nexpt]; // true mean for control group in experiment
  real mu; // mean across experiments
  real<lower=0> sigma; //standard deviation across experiments
}
model {
  // priors
  mu ~ normal(0, 10);
  sigma ~ normal(0, 3);
  // likelihood
  for (i in 1:nexpt) {
	  m[i] ~ normal(mu, sigma);
	  ybar[i] ~ normal(m[i], s[i]/sqrt(nobs[i])); 
  }
}
```


## Fit hierarchical model to past experiments
```{r, include=TRUE, cashe=TRUE}
lr <- read.csv("display_LewisRao2015Retail.csv")
# data taken from tables 1 and 2 of Lewis and Rao (2015)
c <- c(1:3,5:6) # include only advertiser 1 and eliminate exp 4
d1 <- list(nexpt=length(c), nobs=lr$n1[c], ybar=lr$m[c], s=lr$s[c])
m1 <- stan(file="test_roll_model.stan", data=d1, seed=20030601, 
           iter=10000)
```


## Fitted model
```{r}
summary(m1)$summary[,c(1,3,5,8)]
```



# Design Test & Roll -----
# to replicate the exact results in the paper, use these rounded values
mu = 0.68
sigma = 0.03
N = 100000

# Prior on effect size
pdf("website.pdf")
sqrt(2)*sigma/sqrt(pi) # mean effect size
plot_prior_effect_nn(mu, sigma, abs=TRUE)

# Profit-maximizing
(n_star <- test_size_nn(N, s=sqrt(mu*(1-mu)), sigma=sigma))
(eval <- test_eval_nn(n=n_star, N=N, s=sqrt(mu*(1-mu)), mu=mu, sigma=sigma))

# Compare to NHT 
d <- 0.68*0.02 # 2% lift 
(pnorm(d, mean=0, sd = sqrt(2)*sigma)-0.5)*2 # percentile of TE distribution
(n_nht <- test_size_nht(s=sqrt(mu*(1-mu)), d=d)) # to match the profit maximizing
(eval_nht <- test_eval_nn(n=rep(n_nht, 2), N=N, s=sqrt(mu*(1-mu)), mu=mu, sigma=sigma))
eval$profit - eval_nht$profit

# Compare to NHT with Finite Population Correction
(n_fpc <- test_size_nht(s=sqrt(mu*(1-mu)), d=d, N=100000)) 
(eval_fpc <- test_eval_nn(n=rep(n_fpc, 2), N=N, s=sqrt(mu*(1-mu)), mu=mu, sigma=sigma))
eval$profit - eval_fpc$profit

# Profit and error rate as a function of n -----
# Profit
n <- c(1:19, 2:19*10, 2:19*100, 2:19*1000, 2:5*10000)
out <- NULL
for (i in 1:length(n)) {
  out <- rbind(out, test_eval_nn(n=c(n[i], n[i]), N=N, s=sqrt(mu*(1-mu)), mu=mu, sigma=sigma))
}
plot(out$n1, out$profit, type="l", 
     ylim=c(out$profit_rand[1], out$profit_perfect[1]),
     xlab=expression("Test Size (n"[1]*"=n"[2]*")"), ylab="Expected Conversions")
abline(v=n_star)
text(n_star, 0.696*N, "n*=2,284", pos=4)
abline(v=n_fpc, col="red", lty=2)
text(n_fpc, 0.683*N, expression(n[FPC]), col="red", pos=2)  # hard code
abline(v=n_nht, col="red", lty=3)
text(n_nht, 0.683*N, expression(n[HT]), col="red", pos=4)  # hard code

# Error rate
plot(out$n1, out$error_rate, type="l", ylim=c(0, 0.5),
     xlab=expression("Test Size (n"[1]*"=n"[2]*")"), ylab="Error Rate")
abline(v=n_star)
text(n_star, 0.13, "n*=2,284", pos=4)
abline(v=n_fpc, col="red", lty=2)
text(n_fpc, 0.3, expression(n[FPC]), col="red", pos=2)
abline(v=n_nht, col="red", lty=3)
text(n_nht, 0.3, expression(n[HT]), col="red", pos=4)

# Sample size sensitivities -----
cex <- 1
ylim <- c(0,25000)

# Optimal sample sizes for different N 
N <- c(100, (1:10000)*1000)
out <- NULL; fpc <- NULL
for (i in 1:length(N)) {
  out <- c(out, test_size_nn(N[i], s=sqrt(mu*(1-mu)), sigma=sigma)[1])
  fpc <-  c(fpc, test_size_nht(s=sqrt(mu*(1-mu)), d=d, N=N[i])) 
}
plot(N/1000000, out, type="l", ylim=ylim,
     xlab="Population Size (N, millions)", ylab="Test Size", cex.lab=cex)
text(N[2000]/1000000, out[2000], "n*", pos=1, cex=cex)
lines(N/1000000, fpc, col="red", lty=2)
text(N[200]/1000000, fpc[200], expression(n[FPC]), col="red", pos=4, cex=cex)
abline(h=n_nht, col="red", lty=3)
text(N[2000]/1000000, n_nht, expression(n[HT]), col="red", pos=3, cex=cex)

# Optimal sample sizes for different sigma
N=100000
sigma <- seq(0.001, 0.19, 0.001)
d <- qnorm(0.5+0.125, mean=0, sd=sqrt(2)*sigma) #25th percentile of prior on te
out <- NULL; nht <- NULL; fpc <- NULL
for (i in 1:length(sigma)) {
  out <- c(out, test_size_nn(N, s=sqrt(mu*(1-mu)), sigma=sigma[i])[1])
  nht <- c(nht, test_size_nht(s=sqrt(mu*(1-mu)), d=d[i])) 
  fpc <-  c(fpc, test_size_nht(s=sqrt(mu*(1-mu)), d=d[i], N=N)) 
}
plot(sigma, out, type="l", ylim=ylim,
     xlab=expression(paste("StDev of Prior (", sigma, ")", sep="")), 
     ylab="Test Size", cex.lab=1.3, cex.lab=cex)
text(sigma[15], out[15], "n*", pos=1, cex=cex)
lines(sigma, fpc, col="red", lty=2)
text(sigma[30], fpc[30], expression(n[FPC]), col="red", pos=2, cex=cex)
lines(sigma, nht, col="red", lty=3)
text(sigma[30], nht[30], expression(n[HT]), col="red", pos=4, cex=cex)

# Optimal sample sizes for different s
sigma <- 0.03
d <- 0.68*0.02
s <- seq(0.001, 0.7, 0.002)
out <- NULL; nht <- NULL; fpc <- NULL
for (i in 1:length(s)) {
  out <- rbind(out, test_size_nn(N, s=s[i], sigma=sigma))
  nht <- rbind(nht, test_size_nht(s=s[i], d=d))
  fpc <- rbind(fpc, test_size_nht(s=s[i], d=d, N=100000))
}
plot(s, out[,1], type="l", ylim=ylim,
     xlab="StDev of Response (s)", ylab="Test Size", cex.lab=cex)
text(s[250], out[250,1], "n*", pos=3, cex=cex)
lines(s, fpc, type="l", col="red", lty=2)
text(s[300], fpc[300]-700, expression(n[FPC]), col="red", pos=1, cex=cex)
lines(s, nht, type="l", col="red", lty=3)
text(s[250], nht[250], expression(n[HT]), col="red", pos=2, cex=cex)
dev.off()



## Evaluate the test
```{R}
(eval <- test_eval_nn(n=n, N=1000000, s=mean(d1$s), mu=10.36044, sigma=4.3964))
```


## Compare to sample size for hypothesis test 
Null hypothesis test size to detect difference between:  
- display ads that have no effect 
- display ads that are exactly worth the costs 
(ROI = 0 versus ROI = -100).
```{r}
margin <- 0.5
d <- mean(lr$cost[c])/margin
(n_nht <- test_size_nht(s=mean(d1$s), d=d))  
```


## Sample size for hypothesis test with finite population correction
```{r}
(n_fpc <- test_size_nht(s=mean(d1$s), d=d, N=1000000))  
(eval_fpc <- test_eval_nn(c(n_fpc, n_fpc), N=1000000, 
                          s=mean(d1$s), mu=10.36044, sigma=4.39646))
```


 

## Test & Roll procedure
1. Come up with priors distributions for each treatment 
    - Use past data, if you've got it
2. Use the priors to compute the optimal sample size
3. Run the test
4. Deploy the treatment with the higher posterior to the remainder of the population
    - Priors are symmetric $\rightarrow$ pick the treatment with the higher average

## Hypothesis Test versus Test & Roll
### Hypothesis tests are for science
- Want to know which treatment is better with high confidence 
- Don’t explicitly consider the cost of the experiment  
### Test & Roll is for marketing  
- Want to select treatments that maximize profits  
- Considers the cost of the test relative to the error rate 

